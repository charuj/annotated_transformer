# annotated_transformer
Explorations in how transformers work, per the paper "Attention is all you need" 
